# -*- coding: utf-8 -*-
"""MATH_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kh7u-qZvdobjui7sC34t6wsY4hJA4YIw
"""

#!/usr/bin/env python3

import os
import torch
import re
import pandas as pd
from collections import defaultdict
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset, Features, Value
from huggingface_hub import login

HF_TOKEN   = ""
login(token=HF_TOKEN)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Pytorch version is: ", torch.__version__)
print("You are using: ", DEVICE)

"""unzip the MATH dataset and MATH reference examples for few-shot prompting"""

import zipfile

zip_path = "example.zip"
extract_dir = "./examples"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

zip_path = "dataset.zip"
extract_dir = "./data"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-math-7b-instruct",
    token=HF_TOKEN,
    device_map={"": DEVICE},
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)


tokenizer = AutoTokenizer.from_pretrained(
    "deepseek-ai/deepseek-math-7b-instruct",
    token=HF_TOKEN
)

features = Features({
    "problem": Value("string"),
    "level": Value("string"),
    "type": Value("string"),
    "solution": Value("string"),
})

dataset = load_dataset(
    "json",
    data_files={
        "train": "data/MATH/train/**/*.json",
        "test": "data/MATH/test/**/*.json"
    },
    features=features
)


few_shots = load_dataset(
    "json",
    data_files={
        "examples": "examples/examples/*.json"
    },
    features=features
)


samples = defaultdict(list)
for sample in tqdm(few_shots['examples']):
    samples[sample['type']].append(f'sample question:{sample["problem"]}, sample solution: {sample["solution"]}')

"""Generate training dataset on MATH Dataset with baseline zero-shot prompting"""

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

def generate_answer(problem_text):
    prompt = f"""Solve the problem step by step. Use $$ or $$$$ for LaTex. Put the final answer in \\boxed{{}}.


### Problem:
{problem_text}

### Solution:"""
    inputs = tokenizer(prompt, return_tensors="pt",padding=True).to(model.device)
    outputs = model.generate(**inputs,pad_token_id=tokenizer.eos_token_id,  max_new_tokens=2048)
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    solution = full_response.split("### Solution:")[-1].strip()
    solution = re.sub(r'\s*\n\s*', ' ', solution)
    solution = re.sub(r'\s+', ' ', solution)
    return solution


results = []
for example in tqdm(dataset["train"]):
    try:
        generated_answer = generate_answer(example["problem"])
        results.append({
            "problem": example["problem"],
            "type": example["type"],
            "level": example["level"],
            "generated_answer": generated_answer,
            "ground_truth": example['solution']
        })
    except Exception as e:
        print(f"Failed on {example['type']} problem: {str(e)}")

results_df = pd.DataFrame(results)
csv_path = "train_baseline.csv"

results_df.to_csv(
    csv_path,
    mode='a',
    index=False,
    header=not os.path.exists(csv_path)
)

"""Generate test dataset on MATH Dataset with baseline zero-shot prompting"""

def generate_answer(problem_text):
    prompt = f"""Solve the problem step by step. Use $$ or $$$$ for LaTex. Put the final answer in \\boxed{{}}.


### Problem:
{problem_text}

### Solution:"""
    inputs = tokenizer(prompt, return_tensors="pt",padding=True).to(model.device)
    outputs = model.generate(**inputs,pad_token_id=tokenizer.eos_token_id,  max_new_tokens=2048)
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    solution = full_response.split("### Solution:")[-1].strip()
    solution = re.sub(r'\s*\n\s*', ' ', solution)
    solution = re.sub(r'\s+', ' ', solution)
    return solution


results = []
for example in tqdm(dataset["test"]):
    try:
        generated_answer = generate_answer(example["problem"])
        results.append({
            "problem": example["problem"],
            "type": example["type"],
            "level": example["level"],
            "generated_answer": generated_answer,
            "ground_truth": example['solution']
        })
    except Exception as e:
        print(f"Failed on {example['type']} problem: {str(e)}")

results_df = pd.DataFrame(results)
csv_path = "test_baseline.csv"

results_df.to_csv(
    csv_path,
    mode='a',
    index=False,
    header=not os.path.exists(csv_path)
)